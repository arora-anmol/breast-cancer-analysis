
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Untitled}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
        \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{svm}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{naive\PYZus{}bayes} \PY{k}{import} \PY{n}{GaussianNB}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LogisticRegression}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{accuracy\PYZus{}score}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kn}{import} \PY{n+nn}{warnings}
        \PY{n}{warnings}\PY{o}{.}\PY{n}{filterwarnings}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{} disabling warnings for presentation/ bad practice don\PYZsq{}t do it.}
\end{Verbatim}


    

    \hypertarget{summary}{%
\subsection{Summary:}\label{summary}}

\begin{itemize}
\tightlist
\item
  What is Machine learning?
\item
  Types of machine learning and problem classification
\item
  Machine learning pipeline
\item
  Using the breast cancer dataset to solve binary classification problem
\item
  Walk though the entire pipeline step by step using the dataset
\item
  Special emphasis on Modeling (comprises theory for algorithms) with:
\item
  Gaussian Naive Bayes
\item
  Support Vector Machine(SVM)
\item
  Logistic Regression
\item
  Understanding the model after training: overfitting and underfitting
\item
  Model Evaluation
\item
  After cross validation/ testing across different models, recommend the
  best one.
\end{itemize}

    

    \hypertarget{what-is-machine-learning}{%
\subsection{What is Machine Learning?}\label{what-is-machine-learning}}

``Machine learning is the science of getting computers to act without
being explicitly programmed.'' - Stanford

    \hypertarget{different-types-of-machine-learning}{%
\subsection{Different types of Machine
Learning:}\label{different-types-of-machine-learning}}

    \begin{itemize}
\tightlist
\item
  Supervised Learning
\item
  Unsupervised Learning
\item
  Reinforcement Learning
\end{itemize}

    \hypertarget{supervised-learning}{%
\subsubsection{Supervised Learning}\label{supervised-learning}}

    In supervised learning, we are given a data set and already know what
our correct output should look like, having the idea that there is a
relationship between the input and the output. The inputs are the
features, and output are the labels. The two tasks of supervised
learning: regression and classification. In a regression problem, we are
trying to predict results within a continuous output, meaning that we
are trying to map input variables to some continuous function. In a
classification. problem, we are instead trying to predict results in a
discrete output. In other words, we are trying to map input variables
into discrete categories. This also means that we are trying to find a
decision boundary.

    \begin{itemize}
\tightlist
\item
  Examples for classification: Binary classification ( churn/no churn,
  malginant/benign etc), Multi Class - (Classify a handwritten digit
  etc)
\item
  Example for regression: Predicting housing price, stock price etc (
  Continuous )
\end{itemize}

    \hypertarget{unsupervised-learning}{%
\subsubsection{Unsupervised Learning}\label{unsupervised-learning}}

    In unsupervised learning, we approach problems with little or no idea
what our results should look like. We can derive structure from data
where we don't necessarily know the effect of the variables.We can
derive this structure by clustering the data based on relationships
among the variables in the data.

    Examples: Determining different foods in a basket based on certain
features etc

    \hypertarget{reinforcement-learning}{%
\subsubsection{Reinforcement Learning}\label{reinforcement-learning}}

    Reinforcement learning is the training of machine learning models to
make a sequence of decisions. The agent learns to achieve a goal in an
uncertain, potentially complex environment. In reinforcement learning,
an artificial intelligence faces a game-like situation. The computer
employs trial and error to come up with a solution to the problem. To
get the machine to do what the programmer wants, the artificial
intelligence gets either rewards or penalties for the actions it
performs. Its goal is to maximize the total reward.

    Example: PC games where enemy changes moves based on your performance,
robot learning to run etc

    

    \hypertarget{what-does-a-machine-learning-pipeline-look-like}{%
\subsection{What does a Machine Learning pipeline look
like?}\label{what-does-a-machine-learning-pipeline-look-like}}

    

    

    \hypertarget{goal}{%
\subsection{Goal:}\label{goal}}

\hypertarget{look-at-the-breast-cancer-data-from-httpswww.kaggle.comucimlbreast-cancer-wisconsin-data-and-walk-through-the-machine-learning-process-with-more-emphasis-on-modeling-and-how-to-applyvalidate-ml-algorithms}{%
\subsubsection{Look at the breast cancer data from
https://www.kaggle.com/uciml/breast-cancer-wisconsin-data and walk
through the machine learning process, with more emphasis on Modeling and
how to apply/validate ML
algorithms}\label{look-at-the-breast-cancer-data-from-httpswww.kaggle.comucimlbreast-cancer-wisconsin-data-and-walk-through-the-machine-learning-process-with-more-emphasis-on-modeling-and-how-to-applyvalidate-ml-algorithms}}

    

    \hypertarget{loading-the-dataset}{%
\subsection{Loading the dataset:}\label{loading-the-dataset}}

The raw data is already transformed, so we can move to the
pre-processing part directly

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:}          id diagnosis  radius\_mean  texture\_mean  perimeter\_mean  area\_mean  \textbackslash{}
        0    842302         M        17.99         10.38          122.80     1001.0   
        1    842517         M        20.57         17.77          132.90     1326.0   
        2  84300903         M        19.69         21.25          130.00     1203.0   
        3  84348301         M        11.42         20.38           77.58      386.1   
        4  84358402         M        20.29         14.34          135.10     1297.0   
        
           smoothness\_mean  compactness\_mean  concavity\_mean  concave points\_mean  \textbackslash{}
        0          0.11840           0.27760          0.3001              0.14710   
        1          0.08474           0.07864          0.0869              0.07017   
        2          0.10960           0.15990          0.1974              0.12790   
        3          0.14250           0.28390          0.2414              0.10520   
        4          0.10030           0.13280          0.1980              0.10430   
        
              {\ldots}       texture\_worst  perimeter\_worst  area\_worst  smoothness\_worst  \textbackslash{}
        0     {\ldots}               17.33           184.60      2019.0            0.1622   
        1     {\ldots}               23.41           158.80      1956.0            0.1238   
        2     {\ldots}               25.53           152.50      1709.0            0.1444   
        3     {\ldots}               26.50            98.87       567.7            0.2098   
        4     {\ldots}               16.67           152.20      1575.0            0.1374   
        
           compactness\_worst  concavity\_worst  concave points\_worst  symmetry\_worst  \textbackslash{}
        0             0.6656           0.7119                0.2654          0.4601   
        1             0.1866           0.2416                0.1860          0.2750   
        2             0.4245           0.4504                0.2430          0.3613   
        3             0.8663           0.6869                0.2575          0.6638   
        4             0.2050           0.4000                0.1625          0.2364   
        
           fractal\_dimension\_worst  Unnamed: 32  
        0                  0.11890          NaN  
        1                  0.08902          NaN  
        2                  0.08758          NaN  
        3                  0.17300          NaN  
        4                  0.07678          NaN  
        
        [5 rows x 33 columns]
\end{Verbatim}
            
    \hypertarget{data-pre-processing}{%
\subsection{Data pre-processing}\label{data-pre-processing}}

\begin{itemize}
\tightlist
\item
  Dropping unnecessary columns
\item
  Changing Labels from string to binary, changing column names
\item
  Removing/ Filling NaNs, making sure the right datatypes
\item
  Outlier detection/removal
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{df}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 569 entries, 0 to 568
Data columns (total 33 columns):
id                         569 non-null int64
diagnosis                  569 non-null object
radius\_mean                569 non-null float64
texture\_mean               569 non-null float64
perimeter\_mean             569 non-null float64
area\_mean                  569 non-null float64
smoothness\_mean            569 non-null float64
compactness\_mean           569 non-null float64
concavity\_mean             569 non-null float64
concave points\_mean        569 non-null float64
symmetry\_mean              569 non-null float64
fractal\_dimension\_mean     569 non-null float64
radius\_se                  569 non-null float64
texture\_se                 569 non-null float64
perimeter\_se               569 non-null float64
area\_se                    569 non-null float64
smoothness\_se              569 non-null float64
compactness\_se             569 non-null float64
concavity\_se               569 non-null float64
concave points\_se          569 non-null float64
symmetry\_se                569 non-null float64
fractal\_dimension\_se       569 non-null float64
radius\_worst               569 non-null float64
texture\_worst              569 non-null float64
perimeter\_worst            569 non-null float64
area\_worst                 569 non-null float64
smoothness\_worst           569 non-null float64
compactness\_worst          569 non-null float64
concavity\_worst            569 non-null float64
concave points\_worst       569 non-null float64
symmetry\_worst             569 non-null float64
fractal\_dimension\_worst    569 non-null float64
Unnamed: 32                0 non-null float64
dtypes: float64(31), int64(1), object(1)
memory usage: 146.8+ KB

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Unnamed: 32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k}{def} \PY{n+nf}{replace\PYZus{}label}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
             \PY{k}{if} \PY{n}{x} \PY{o+ow}{is} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                 \PY{k}{return} \PY{l+m+mi}{1}
             \PY{k}{else}\PY{p}{:}
                 \PY{k}{return} \PY{l+m+mi}{0}
             
         \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{diagnosis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{diagnosis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{replace\PYZus{}label}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{df}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}11}]:}         diagnosis  radius\_mean  texture\_mean  perimeter\_mean    area\_mean  \textbackslash{}
         count  569.000000   569.000000    569.000000      569.000000   569.000000   
         mean     0.372583    14.127292     19.289649       91.969033   654.889104   
         std      0.483918     3.524049      4.301036       24.298981   351.914129   
         min      0.000000     6.981000      9.710000       43.790000   143.500000   
         25\%      0.000000    11.700000     16.170000       75.170000   420.300000   
         50\%      0.000000    13.370000     18.840000       86.240000   551.100000   
         75\%      1.000000    15.780000     21.800000      104.100000   782.700000   
         max      1.000000    28.110000     39.280000      188.500000  2501.000000   
         
                smoothness\_mean  compactness\_mean  concavity\_mean  concave points\_mean  \textbackslash{}
         count       569.000000        569.000000      569.000000           569.000000   
         mean          0.096360          0.104341        0.088799             0.048919   
         std           0.014064          0.052813        0.079720             0.038803   
         min           0.052630          0.019380        0.000000             0.000000   
         25\%           0.086370          0.064920        0.029560             0.020310   
         50\%           0.095870          0.092630        0.061540             0.033500   
         75\%           0.105300          0.130400        0.130700             0.074000   
         max           0.163400          0.345400        0.426800             0.201200   
         
                symmetry\_mean           {\ldots}             radius\_worst  texture\_worst  \textbackslash{}
         count     569.000000           {\ldots}               569.000000     569.000000   
         mean        0.181162           {\ldots}                16.269190      25.677223   
         std         0.027414           {\ldots}                 4.833242       6.146258   
         min         0.106000           {\ldots}                 7.930000      12.020000   
         25\%         0.161900           {\ldots}                13.010000      21.080000   
         50\%         0.179200           {\ldots}                14.970000      25.410000   
         75\%         0.195700           {\ldots}                18.790000      29.720000   
         max         0.304000           {\ldots}                36.040000      49.540000   
         
                perimeter\_worst   area\_worst  smoothness\_worst  compactness\_worst  \textbackslash{}
         count       569.000000   569.000000        569.000000         569.000000   
         mean        107.261213   880.583128          0.132369           0.254265   
         std          33.602542   569.356993          0.022832           0.157336   
         min          50.410000   185.200000          0.071170           0.027290   
         25\%          84.110000   515.300000          0.116600           0.147200   
         50\%          97.660000   686.500000          0.131300           0.211900   
         75\%         125.400000  1084.000000          0.146000           0.339100   
         max         251.200000  4254.000000          0.222600           1.058000   
         
                concavity\_worst  concave points\_worst  symmetry\_worst  \textbackslash{}
         count       569.000000            569.000000      569.000000   
         mean          0.272188              0.114606        0.290076   
         std           0.208624              0.065732        0.061867   
         min           0.000000              0.000000        0.156500   
         25\%           0.114500              0.064930        0.250400   
         50\%           0.226700              0.099930        0.282200   
         75\%           0.382900              0.161400        0.317900   
         max           1.252000              0.291000        0.663800   
         
                fractal\_dimension\_worst  
         count               569.000000  
         mean                  0.083946  
         std                   0.018061  
         min                   0.055040  
         25\%                   0.071460  
         50\%                   0.080040  
         75\%                   0.092080  
         max                   0.207500  
         
         [8 rows x 31 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n}{df}\PY{o}{.}\PY{n}{columns}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}12}]:} Index(['diagnosis', 'radius\_mean', 'texture\_mean', 'perimeter\_mean',
                'area\_mean', 'smoothness\_mean', 'compactness\_mean', 'concavity\_mean',
                'concave points\_mean', 'symmetry\_mean', 'fractal\_dimension\_mean',
                'radius\_se', 'texture\_se', 'perimeter\_se', 'area\_se', 'smoothness\_se',
                'compactness\_se', 'concavity\_se', 'concave points\_se', 'symmetry\_se',
                'fractal\_dimension\_se', 'radius\_worst', 'texture\_worst',
                'perimeter\_worst', 'area\_worst', 'smoothness\_worst',
                'compactness\_worst', 'concavity\_worst', 'concave points\_worst',
                'symmetry\_worst', 'fractal\_dimension\_worst'],
               dtype='object')
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{standard\PYZus{}error\PYZus{}columns} \PY{o}{=} \PY{p}{[}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{df}\PY{o}{.}\PY{n}{columns} \PY{k}{if} \PY{n}{i}\PY{o}{.}\PY{n}{endswith}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}se}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns} \PY{o}{=} \PY{n}{standard\PYZus{}error\PYZus{}columns}\PY{p}{,} \PY{n}{inplace} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)} 
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{df}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}15}]:} (569, 21)
\end{Verbatim}
            
    \hypertarget{feature-extraction}{%
\subsection{Feature Extraction}\label{feature-extraction}}

\begin{itemize}
\tightlist
\item
  Use domain knowledge to select features, if any
\item
  Check for correlation with churn markers
\item
  Remove highly correlated variables, as we want independent features
\item
  Understanding if data is linearly seperable
\item
  Check if dataset is balanced
\item
  Focus on understanding the dataset/ features
\item
  Most time consuming for solving a real life problem
\end{itemize}

    \hypertarget{correlation-matrix}{%
\subsubsection{Correlation Matrix:}\label{correlation-matrix}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{df}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:}                          diagnosis  radius\_mean  texture\_mean  perimeter\_mean  \textbackslash{}
         diagnosis                 1.000000     0.730029      0.415185        0.742636   
         radius\_mean               0.730029     1.000000      0.323782        0.997855   
         texture\_mean              0.415185     0.323782      1.000000        0.329533   
         perimeter\_mean            0.742636     0.997855      0.329533        1.000000   
         area\_mean                 0.708984     0.987357      0.321086        0.986507   
         smoothness\_mean           0.358560     0.170581      0.023389        0.207278   
         compactness\_mean          0.596534     0.506124      0.236702        0.556936   
         concavity\_mean            0.696360     0.676764      0.302418        0.716136   
         concave points\_mean       0.776614     0.822529      0.293464        0.850977   
         symmetry\_mean             0.330499     0.147741      0.071401        0.183027   
         fractal\_dimension\_mean    0.012838     0.311631      0.076437        0.261477   
         radius\_worst              0.776454     0.969539      0.352573        0.969476   
         texture\_worst             0.456903     0.297008      0.912045        0.303038   
         perimeter\_worst           0.782914     0.965137      0.358040        0.970387   
         area\_worst                0.733825     0.941082      0.343546        0.941550   
         smoothness\_worst          0.421465     0.119616      0.077503        0.150549   
         compactness\_worst         0.590998     0.413463      0.277830        0.455774   
         concavity\_worst           0.659610     0.526911      0.301025        0.563879   
         concave points\_worst      0.793566     0.744214      0.295316        0.771241   
         symmetry\_worst            0.416294     0.163953      0.105008        0.189115   
         fractal\_dimension\_worst   0.323872     0.007066      0.119205        0.051019   
         
                                  area\_mean  smoothness\_mean  compactness\_mean  \textbackslash{}
         diagnosis                 0.708984         0.358560          0.596534   
         radius\_mean               0.987357         0.170581          0.506124   
         texture\_mean              0.321086         0.023389          0.236702   
         perimeter\_mean            0.986507         0.207278          0.556936   
         area\_mean                 1.000000         0.177028          0.498502   
         smoothness\_mean           0.177028         1.000000          0.659123   
         compactness\_mean          0.498502         0.659123          1.000000   
         concavity\_mean            0.685983         0.521984          0.883121   
         concave points\_mean       0.823269         0.553695          0.831135   
         symmetry\_mean             0.151293         0.557775          0.602641   
         fractal\_dimension\_mean    0.283110         0.584792          0.565369   
         radius\_worst              0.962746         0.213120          0.535315   
         texture\_worst             0.287489         0.036072          0.248133   
         perimeter\_worst           0.959120         0.238853          0.590210   
         area\_worst                0.959213         0.206718          0.509604   
         smoothness\_worst          0.123523         0.805324          0.565541   
         compactness\_worst         0.390410         0.472468          0.865809   
         concavity\_worst           0.512606         0.434926          0.816275   
         concave points\_worst      0.722017         0.503053          0.815573   
         symmetry\_worst            0.143570         0.394309          0.510223   
         fractal\_dimension\_worst   0.003738         0.499316          0.687382   
         
                                  concavity\_mean  concave points\_mean  symmetry\_mean  \textbackslash{}
         diagnosis                      0.696360             0.776614       0.330499   
         radius\_mean                    0.676764             0.822529       0.147741   
         texture\_mean                   0.302418             0.293464       0.071401   
         perimeter\_mean                 0.716136             0.850977       0.183027   
         area\_mean                      0.685983             0.823269       0.151293   
         smoothness\_mean                0.521984             0.553695       0.557775   
         compactness\_mean               0.883121             0.831135       0.602641   
         concavity\_mean                 1.000000             0.921391       0.500667   
         concave points\_mean            0.921391             1.000000       0.462497   
         symmetry\_mean                  0.500667             0.462497       1.000000   
         fractal\_dimension\_mean         0.336783             0.166917       0.479921   
         radius\_worst                   0.688236             0.830318       0.185728   
         texture\_worst                  0.299879             0.292752       0.090651   
         perimeter\_worst                0.729565             0.855923       0.219169   
         area\_worst                     0.675987             0.809630       0.177193   
         smoothness\_worst               0.448822             0.452753       0.426675   
         compactness\_worst              0.754968             0.667454       0.473200   
         concavity\_worst                0.884103             0.752399       0.433721   
         concave points\_worst           0.861323             0.910155       0.430297   
         symmetry\_worst                 0.409464             0.375744       0.699826   
         fractal\_dimension\_worst        0.514930             0.368661       0.438413   
         
                                           {\ldots}             radius\_worst  texture\_worst  \textbackslash{}
         diagnosis                         {\ldots}                 0.776454       0.456903   
         radius\_mean                       {\ldots}                 0.969539       0.297008   
         texture\_mean                      {\ldots}                 0.352573       0.912045   
         perimeter\_mean                    {\ldots}                 0.969476       0.303038   
         area\_mean                         {\ldots}                 0.962746       0.287489   
         smoothness\_mean                   {\ldots}                 0.213120       0.036072   
         compactness\_mean                  {\ldots}                 0.535315       0.248133   
         concavity\_mean                    {\ldots}                 0.688236       0.299879   
         concave points\_mean               {\ldots}                 0.830318       0.292752   
         symmetry\_mean                     {\ldots}                 0.185728       0.090651   
         fractal\_dimension\_mean            {\ldots}                 0.253691       0.051269   
         radius\_worst                      {\ldots}                 1.000000       0.359921   
         texture\_worst                     {\ldots}                 0.359921       1.000000   
         perimeter\_worst                   {\ldots}                 0.993708       0.365098   
         area\_worst                        {\ldots}                 0.984015       0.345842   
         smoothness\_worst                  {\ldots}                 0.216574       0.225429   
         compactness\_worst                 {\ldots}                 0.475820       0.360832   
         concavity\_worst                   {\ldots}                 0.573975       0.368366   
         concave points\_worst              {\ldots}                 0.787424       0.359755   
         symmetry\_worst                    {\ldots}                 0.243529       0.233027   
         fractal\_dimension\_worst           {\ldots}                 0.093492       0.219122   
         
                                  perimeter\_worst  area\_worst  smoothness\_worst  \textbackslash{}
         diagnosis                       0.782914    0.733825          0.421465   
         radius\_mean                     0.965137    0.941082          0.119616   
         texture\_mean                    0.358040    0.343546          0.077503   
         perimeter\_mean                  0.970387    0.941550          0.150549   
         area\_mean                       0.959120    0.959213          0.123523   
         smoothness\_mean                 0.238853    0.206718          0.805324   
         compactness\_mean                0.590210    0.509604          0.565541   
         concavity\_mean                  0.729565    0.675987          0.448822   
         concave points\_mean             0.855923    0.809630          0.452753   
         symmetry\_mean                   0.219169    0.177193          0.426675   
         fractal\_dimension\_mean          0.205151    0.231854          0.504942   
         radius\_worst                    0.993708    0.984015          0.216574   
         texture\_worst                   0.365098    0.345842          0.225429   
         perimeter\_worst                 1.000000    0.977578          0.236775   
         area\_worst                      0.977578    1.000000          0.209145   
         smoothness\_worst                0.236775    0.209145          1.000000   
         compactness\_worst               0.529408    0.438296          0.568187   
         concavity\_worst                 0.618344    0.543331          0.518523   
         concave points\_worst            0.816322    0.747419          0.547691   
         symmetry\_worst                  0.269493    0.209146          0.493838   
         fractal\_dimension\_worst         0.138957    0.079647          0.617624   
         
                                  compactness\_worst  concavity\_worst  \textbackslash{}
         diagnosis                         0.590998         0.659610   
         radius\_mean                       0.413463         0.526911   
         texture\_mean                      0.277830         0.301025   
         perimeter\_mean                    0.455774         0.563879   
         area\_mean                         0.390410         0.512606   
         smoothness\_mean                   0.472468         0.434926   
         compactness\_mean                  0.865809         0.816275   
         concavity\_mean                    0.754968         0.884103   
         concave points\_mean               0.667454         0.752399   
         symmetry\_mean                     0.473200         0.433721   
         fractal\_dimension\_mean            0.458798         0.346234   
         radius\_worst                      0.475820         0.573975   
         texture\_worst                     0.360832         0.368366   
         perimeter\_worst                   0.529408         0.618344   
         area\_worst                        0.438296         0.543331   
         smoothness\_worst                  0.568187         0.518523   
         compactness\_worst                 1.000000         0.892261   
         concavity\_worst                   0.892261         1.000000   
         concave points\_worst              0.801080         0.855434   
         symmetry\_worst                    0.614441         0.532520   
         fractal\_dimension\_worst           0.810455         0.686511   
         
                                  concave points\_worst  symmetry\_worst  \textbackslash{}
         diagnosis                            0.793566        0.416294   
         radius\_mean                          0.744214        0.163953   
         texture\_mean                         0.295316        0.105008   
         perimeter\_mean                       0.771241        0.189115   
         area\_mean                            0.722017        0.143570   
         smoothness\_mean                      0.503053        0.394309   
         compactness\_mean                     0.815573        0.510223   
         concavity\_mean                       0.861323        0.409464   
         concave points\_mean                  0.910155        0.375744   
         symmetry\_mean                        0.430297        0.699826   
         fractal\_dimension\_mean               0.175325        0.334019   
         radius\_worst                         0.787424        0.243529   
         texture\_worst                        0.359755        0.233027   
         perimeter\_worst                      0.816322        0.269493   
         area\_worst                           0.747419        0.209146   
         smoothness\_worst                     0.547691        0.493838   
         compactness\_worst                    0.801080        0.614441   
         concavity\_worst                      0.855434        0.532520   
         concave points\_worst                 1.000000        0.502528   
         symmetry\_worst                       0.502528        1.000000   
         fractal\_dimension\_worst              0.511114        0.537848   
         
                                  fractal\_dimension\_worst  
         diagnosis                               0.323872  
         radius\_mean                             0.007066  
         texture\_mean                            0.119205  
         perimeter\_mean                          0.051019  
         area\_mean                               0.003738  
         smoothness\_mean                         0.499316  
         compactness\_mean                        0.687382  
         concavity\_mean                          0.514930  
         concave points\_mean                     0.368661  
         symmetry\_mean                           0.438413  
         fractal\_dimension\_mean                  0.767297  
         radius\_worst                            0.093492  
         texture\_worst                           0.219122  
         perimeter\_worst                         0.138957  
         area\_worst                              0.079647  
         smoothness\_worst                        0.617624  
         compactness\_worst                       0.810455  
         concavity\_worst                         0.686511  
         concave points\_worst                    0.511114  
         symmetry\_worst                          0.537848  
         fractal\_dimension\_worst                 1.000000  
         
         [21 rows x 21 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
         \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Blues}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{annot}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)} 
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}19}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x174f726d438>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_40_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{} Create correlation matrix}
         \PY{n}{corr\PYZus{}matrix} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Select upper triangle of correlation matrix}
         \PY{n}{upper} \PY{o}{=} \PY{n}{corr\PYZus{}matrix}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{triu}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{n}{corr\PYZus{}matrix}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{bool}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Find index of feature columns with correlation greater than 0.90}
         \PY{n}{to\PYZus{}drop} \PY{o}{=} \PY{p}{[}\PY{n}{column} \PY{k}{for} \PY{n}{column} \PY{o+ow}{in} \PY{n}{upper}\PY{o}{.}\PY{n}{columns} \PY{k}{if} \PY{n+nb}{any}\PY{p}{(}\PY{n}{upper}\PY{p}{[}\PY{n}{column}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mf}{0.90}\PY{p}{)}\PY{p}{]}
         \PY{n}{to\PYZus{}drop}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}20}]:} ['perimeter\_mean',
          'area\_mean',
          'concave points\_mean',
          'radius\_worst',
          'texture\_worst',
          'perimeter\_worst',
          'area\_worst',
          'concave points\_worst']
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns} \PY{o}{=} \PY{n}{to\PYZus{}drop}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{df}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}22}]:} (569, 13)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n}{df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}23}]:}    diagnosis  radius\_mean  texture\_mean  smoothness\_mean  compactness\_mean  \textbackslash{}
         0          1        17.99         10.38          0.11840           0.27760   
         1          1        20.57         17.77          0.08474           0.07864   
         2          1        19.69         21.25          0.10960           0.15990   
         3          1        11.42         20.38          0.14250           0.28390   
         4          1        20.29         14.34          0.10030           0.13280   
         
            concavity\_mean  symmetry\_mean  fractal\_dimension\_mean  smoothness\_worst  \textbackslash{}
         0          0.3001         0.2419                 0.07871            0.1622   
         1          0.0869         0.1812                 0.05667            0.1238   
         2          0.1974         0.2069                 0.05999            0.1444   
         3          0.2414         0.2597                 0.09744            0.2098   
         4          0.1980         0.1809                 0.05883            0.1374   
         
            compactness\_worst  concavity\_worst  symmetry\_worst  fractal\_dimension\_worst  
         0             0.6656           0.7119          0.4601                  0.11890  
         1             0.1866           0.2416          0.2750                  0.08902  
         2             0.4245           0.4504          0.3613                  0.08758  
         3             0.8663           0.6869          0.6638                  0.17300  
         4             0.2050           0.4000          0.2364                  0.07678  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
         \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Blues}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{annot}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)} 
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}24}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x174f7308400>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{df}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}25}]:}         diagnosis  radius\_mean  texture\_mean  smoothness\_mean  \textbackslash{}
         count  569.000000   569.000000    569.000000       569.000000   
         mean     0.372583    14.127292     19.289649         0.096360   
         std      0.483918     3.524049      4.301036         0.014064   
         min      0.000000     6.981000      9.710000         0.052630   
         25\%      0.000000    11.700000     16.170000         0.086370   
         50\%      0.000000    13.370000     18.840000         0.095870   
         75\%      1.000000    15.780000     21.800000         0.105300   
         max      1.000000    28.110000     39.280000         0.163400   
         
                compactness\_mean  concavity\_mean  symmetry\_mean  \textbackslash{}
         count        569.000000      569.000000     569.000000   
         mean           0.104341        0.088799       0.181162   
         std            0.052813        0.079720       0.027414   
         min            0.019380        0.000000       0.106000   
         25\%            0.064920        0.029560       0.161900   
         50\%            0.092630        0.061540       0.179200   
         75\%            0.130400        0.130700       0.195700   
         max            0.345400        0.426800       0.304000   
         
                fractal\_dimension\_mean  smoothness\_worst  compactness\_worst  \textbackslash{}
         count              569.000000        569.000000         569.000000   
         mean                 0.062798          0.132369           0.254265   
         std                  0.007060          0.022832           0.157336   
         min                  0.049960          0.071170           0.027290   
         25\%                  0.057700          0.116600           0.147200   
         50\%                  0.061540          0.131300           0.211900   
         75\%                  0.066120          0.146000           0.339100   
         max                  0.097440          0.222600           1.058000   
         
                concavity\_worst  symmetry\_worst  fractal\_dimension\_worst  
         count       569.000000      569.000000               569.000000  
         mean          0.272188        0.290076                 0.083946  
         std           0.208624        0.061867                 0.018061  
         min           0.000000        0.156500                 0.055040  
         25\%           0.114500        0.250400                 0.071460  
         50\%           0.226700        0.282200                 0.080040  
         75\%           0.382900        0.317900                 0.092080  
         max           1.252000        0.663800                 0.207500  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{c+c1}{\PYZsh{} Need to add some box plot to understand/detect outliers}
\end{Verbatim}


    \hypertarget{some-plots-to-understand-the-distribution-better}{%
\subsection{Some plots to understand the distribution
better}\label{some-plots-to-understand-the-distribution-better}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{radius\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{diagnosis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_49_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{compactness\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{diagnosis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_50_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{n}{df\PYZus{}b} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{diagnosis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{n}{df\PYZus{}m} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{diagnosis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{df\PYZus{}b}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{compactness\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df\PYZus{}b}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{radius\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{df\PYZus{}m}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{compactness\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df\PYZus{}m}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{radius\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{m}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_53_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{df\PYZus{}b}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{symmetry\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df\PYZus{}b}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{radius\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{df\PYZus{}m}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{symmetry\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df\PYZus{}m}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{radius\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{m}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_54_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{df\PYZus{}b}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{symmetry\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df\PYZus{}b}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fractal\PYZus{}dimension\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{df\PYZus{}m}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{symmetry\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df\PYZus{}m}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fractal\PYZus{}dimension\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{m}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_55_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{df\PYZus{}b}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fractal\PYZus{}dimension\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df\PYZus{}b}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{radius\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{df\PYZus{}m}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fractal\PYZus{}dimension\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df\PYZus{}m}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{radius\PYZus{}mean}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{m}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_56_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{n}{labels} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{diagnosis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{diagnosis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \hypertarget{making-sure-the-dataset-is-balanced-but-why-deciding-what-metric-to-use-for-model-evaluation}{%
\subsubsection{Making sure the dataset is balanced but why? Deciding
what metric to use for model
evaluation}\label{making-sure-the-dataset-is-balanced-but-why-deciding-what-metric-to-use-for-model-evaluation}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{n}{df\PYZus{}b}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}36}]:} (357, 13)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{n}{df\PYZus{}m}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}37}]:} (212, 13)
\end{Verbatim}
            
    

    \hypertarget{modeling-and-cross-validation}{%
\subsection{Modeling and Cross
Validation}\label{modeling-and-cross-validation}}

\begin{itemize}
\tightlist
\item
  Doing the train-test split
\item
  Building classifiers
\item
  Hyperparameter tuning
\item
  Testing/ Cross validation models using GridSeachCV and K-Fold cross
  validation
\end{itemize}

    Our goal for classification is to find a decision boundary that best
classifies each of the classes. For example if the two classes are
represented by blue circle and red cross respectively, then:
\includegraphics{attachment:image.png}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{x\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{df}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
\end{Verbatim}


    \hypertarget{model-generalization-underfitting-vs-overfitting}{%
\subsection{Model Generalization: Underfitting vs
Overfitting}\label{model-generalization-underfitting-vs-overfitting}}

In machine learning, our goal is to generalize our model to predict
results for unseen data. Real life data has noise/outliers/ and is not
perfect.

What is Underfitting? It refers to a model that can neither model the
training data nor generalize to new data. It's not suitable as it will
have very poor performance.

What is Overfitting? It refers to a model that models the training data
too well. But what does this mean? It means that our model is not
generalized for new training data, and is tightly fit to the training
data.

\hypertarget{but-how-can-we-prevent-this-k-fold-cross-validation-or-resample}{%
\subsubsection{But how can we prevent this? K-Fold Cross Validation or
Resample}\label{but-how-can-we-prevent-this-k-fold-cross-validation-or-resample}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} making sure the train test split}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}39}]:} 0    269
         1    157
         Name: diagnosis, dtype: int64
\end{Verbatim}
            
    \hypertarget{first-algorithm-gaussian-naive-bayes}{%
\subsection{First Algorithm: Gaussian Naive
Bayes}\label{first-algorithm-gaussian-naive-bayes}}

    \hypertarget{what-is-naive-bayes}{%
\paragraph{What is Naive Bayes?}\label{what-is-naive-bayes}}

    \begin{itemize}
\tightlist
\item
  Based on the Bayes Rule
\item
  It assumes that all features are independent of each other (not
  correlated)
\item
  Use the sunny/ rainy/ car/ bike/ being late or not late example
\item
  Naive because of the independence assumption
\item
  Make use of probability distribution for continuous data (gaussian
  here)
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n}{clf\PYZus{}naive\PYZus{}bayes} \PY{o}{=} \PY{n}{GaussianNB}\PY{p}{(}\PY{p}{)} 
\end{Verbatim}


    \hypertarget{hyperparameters}{%
\subsubsection{Hyperparameters:}\label{hyperparameters}}

\begin{itemize}
\tightlist
\item
  priors: prior probability of labels if we don't want to use the ones
  calculated from data'
\item
  var\_smoothing: added to variances for calculation stability - default
  - 1e-9
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{n}{clf\PYZus{}naive\PYZus{}bayes}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}41}]:} GaussianNB(priors=None, var\_smoothing=1e-09)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{clf\PYZus{}naive\PYZus{}bayes}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{n}{accuracy\PYZus{}naive\PYZus{}bayes} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
         \PY{n}{accuracy\PYZus{}naive\PYZus{}bayes}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}43}]:} 0.9020979020979021
\end{Verbatim}
            
    \hypertarget{we-will-use-sklearn.cross_validation-to-do-k-fold-cross-validation-as-we-dont-need-hyperparams-tuning}{%
\subsubsection{We will use sklearn.cross\_validation to do K-Fold cross
validation, as we don't need hyperparams
tuning}\label{we-will-use-sklearn.cross_validation-to-do-k-fold-cross-validation-as-we-dont-need-hyperparams-tuning}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{cross\PYZus{}val\PYZus{}score}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{n}{bayes\PYZus{}cross\PYZus{}val\PYZus{}scores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{clf\PYZus{}naive\PYZus{}bayes}\PY{p}{,} \PY{n}{df}\PY{p}{,} \PY{n}{labels}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{bayes\PYZus{}cross\PYZus{}val\PYZus{}scores}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}45}]:} array([0.9137931 , 0.87931034, 0.87719298, 0.89473684, 0.87719298,
                0.94736842, 0.87719298, 0.94642857, 0.875     , 0.92857143])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{n}{bayes\PYZus{}cross\PYZus{}val\PYZus{}scores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}46}]:} 0.9016787658802178
\end{Verbatim}
            
    \hypertarget{the-accuracy-score-for-gaussian-naive-bayes-is-0.90}{%
\subsubsection{The accuracy score for Gaussian Naive Bayes is
0.90}\label{the-accuracy-score-for-gaussian-naive-bayes-is-0.90}}

    \hypertarget{second-algorithm-support-vector-machinesvm}{%
\subsection{Second Algorithm: Support Vector
Machine(SVM)}\label{second-algorithm-support-vector-machinesvm}}

    What is SVM? \includegraphics{attachment:image.png}

    In SVM, the goal is to find a hyperplane that seperates the two classes.
The hyperplane forms the decision boundary. This is done by maximizing
the margin between them, using the the 2 vectors for each class as
support vectors, hence the name.

    

    It may happen that our data is not linearly seperable, so we can't find
a linear plane to seperate the data. In such a case, what we can use is
the Kernel Trick . Let our dataset look something like this:

    

    This is not linearly separable in the x, y plane. But what if we define
z = x + y

    

    In this, we can see that now our data is linearly separable as we added
another dimension, or looked at the polynomial representation of the
vectors. There's multiple kernels that can be used like rbf, poly,
linear etc

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{n}{clf\PYZus{}svm} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{SVC}\PY{p}{(}\PY{n}{C} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \hypertarget{hyperparameters}{%
\subsubsection{Hyperparameters:}\label{hyperparameters}}

\begin{itemize}
\tightlist
\item
  C: This regularization parameter is used for generalizing the model,
  and controls the trade off between fitting a small margin hyperplane/
  fitting more points, compared to large margin hyperplane/generalizing
  better.
\item
  kernel: This is part of the kernel trick ( linear, rbf, sigmoid,
  polynomial)
\item
  gamma: This is intuitively the impact of points closer to decision
  boundary compared to the ones that are far.
\end{itemize}

    \hypertarget{soft-margin-svm-vs-hard-margin-svm}{%
\subsubsection{Soft Margin SVM vs Hard Margin
SVM}\label{soft-margin-svm-vs-hard-margin-svm}}

    In real life, the data is noisy. We are expected to have some
overlaps/outliers that we should not try to fit. Even if the data was
linearly separable, we can't use hard margin svm because of the overlap.
Thus, we make use of Soft Margin SVM. This svm makes use of a parameter
C, which basically defines how the SVM will handle errors.

    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{n}{clf\PYZus{}svm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)} \PY{c+c1}{\PYZsh{} rbf kernel by default}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}48}]:} SVC(C=1, cache\_size=200, class\_weight=None, coef0=0.0,
           decision\_function\_shape='ovr', degree=3, gamma='auto', kernel='rbf',
           max\_iter=-1, probability=False, random\_state=None, shrinking=True,
           tol=0.001, verbose=False)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{clf\PYZus{}svm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{p}{)}
         \PY{n}{accuracy\PYZus{}rbf\PYZus{}svm} \PY{o}{=} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{n}{accuracy\PYZus{}rbf\PYZus{}svm}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}50}]:} 0.8951048951048951
\end{Verbatim}
            
    \hypertarget{we-are-going-to-use-gridsearchcv-for-hyperparameter-tuning-and-cross-validation}{%
\subsubsection{We are going to use GridSearchCV for hyperparameter
tuning and cross
validation}\label{we-are-going-to-use-gridsearchcv-for-hyperparameter-tuning-and-cross-validation}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{[}
             \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{10000}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kernel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{,}
             \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{,} \PY{l+m+mi}{10000}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gamma}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.001}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kernel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{poly}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{\PYZcb{}}
         \PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}128}]:} \PY{n}{grid} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{clf\PYZus{}svm}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}jobs} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}129}]:} \PY{n}{grid}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{df}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}129}]:} GridSearchCV(cv=10, error\_score='raise-deprecating',
                 estimator=SVC(C=1, cache\_size=200, class\_weight=None, coef0=0.0,
            decision\_function\_shape='ovr', degree=3, gamma='auto', kernel='rbf',
            max\_iter=-1, probability=False, random\_state=None, shrinking=True,
            tol=0.001, verbose=False),
                 fit\_params=None, iid='warn', n\_jobs=-1,
                 param\_grid=[\{'C': [1, 10, 100, 1000, 10000], 'kernel': ['linear']\}, \{'C': [1, 10, 100, 1000, 10000], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['rbf', 'poly']\}],
                 pre\_dispatch='2*n\_jobs', refit=True, return\_train\_score='warn',
                 scoring='accuracy', verbose=0)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}130}]:} \PY{n}{validation\PYZus{}results} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{grid}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{)}
          \PY{n}{validation\PYZus{}results}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}130}]:}    mean\_fit\_time  std\_fit\_time  mean\_score\_time  std\_score\_time param\_C  \textbackslash{}
          0       0.012856      0.001336         0.002838        0.002327       1   
          1       0.057116      0.017294         0.001420        0.000461      10   
          2       0.363571      0.169363         0.001465        0.000489     100   
          3       3.118653      1.291629         0.001367        0.000479    1000   
          4      16.486190      5.262552         0.001172        0.000390   10000   
          
            param\_kernel param\_gamma                            params  \textbackslash{}
          0       linear         NaN      \{'C': 1, 'kernel': 'linear'\}   
          1       linear         NaN     \{'C': 10, 'kernel': 'linear'\}   
          2       linear         NaN    \{'C': 100, 'kernel': 'linear'\}   
          3       linear         NaN   \{'C': 1000, 'kernel': 'linear'\}   
          4       linear         NaN  \{'C': 10000, 'kernel': 'linear'\}   
          
             split0\_test\_score  split1\_test\_score       {\ldots}         split2\_train\_score  \textbackslash{}
          0           0.931034           0.862069       {\ldots}                   0.943359   
          1           0.965517           0.931034       {\ldots}                   0.966797   
          2           0.982759           0.948276       {\ldots}                   0.972656   
          3           0.982759           0.913793       {\ldots}                   0.972656   
          4           0.948276           0.896552       {\ldots}                   0.966797   
          
             split3\_train\_score  split4\_train\_score  split5\_train\_score  \textbackslash{}
          0            0.941406            0.939453            0.939453   
          1            0.957031            0.962891            0.962891   
          2            0.972656            0.970703            0.964844   
          3            0.972656            0.970703            0.972656   
          4            0.972656            0.968750            0.972656   
          
             split6\_train\_score  split7\_train\_score  split8\_train\_score  \textbackslash{}
          0            0.947266            0.939571            0.947368   
          1            0.968750            0.961014            0.959064   
          2            0.972656            0.964912            0.968811   
          3            0.974609            0.968811            0.972710   
          4            0.974609            0.966862            0.972710   
          
             split9\_train\_score  mean\_train\_score  std\_train\_score  
          0            0.947368          0.943761         0.003352  
          1            0.962963          0.963095         0.003412  
          2            0.970760          0.969342         0.002902  
          3            0.972710          0.972467         0.001603  
          4            0.970760          0.970709         0.002616  
          
          [5 rows x 33 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}131}]:} \PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}score\PYZus{}}\PY{p}{,} \PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{,} \PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}131}]:} (0.9701230228471002,
           \{'C': 100, 'gamma': 0.01, 'kernel': 'poly'\},
           SVC(C=100, cache\_size=200, class\_weight=None, coef0=0.0,
             decision\_function\_shape='ovr', degree=3, gamma=0.01, kernel='poly',
             max\_iter=-1, probability=False, random\_state=None, shrinking=True,
             tol=0.001, verbose=False))
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}132}]:} \PY{n}{validation\PYZus{}results}\PY{o}{.}\PY{n}{columns}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}132}]:} Index(['mean\_fit\_time', 'std\_fit\_time', 'mean\_score\_time', 'std\_score\_time',
                 'param\_C', 'param\_kernel', 'param\_gamma', 'params', 'split0\_test\_score',
                 'split1\_test\_score', 'split2\_test\_score', 'split3\_test\_score',
                 'split4\_test\_score', 'split5\_test\_score', 'split6\_test\_score',
                 'split7\_test\_score', 'split8\_test\_score', 'split9\_test\_score',
                 'mean\_test\_score', 'std\_test\_score', 'rank\_test\_score',
                 'split0\_train\_score', 'split1\_train\_score', 'split2\_train\_score',
                 'split3\_train\_score', 'split4\_train\_score', 'split5\_train\_score',
                 'split6\_train\_score', 'split7\_train\_score', 'split8\_train\_score',
                 'split9\_train\_score', 'mean\_train\_score', 'std\_train\_score'],
                dtype='object')
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}133}]:} \PY{n}{validation\PYZus{}results}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{param\PYZus{}kernel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{param\PYZus{}C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{param\PYZus{}gamma}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}train\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}133}]:}    param\_kernel param\_C  mean\_test\_score param\_gamma  mean\_train\_score
          20         poly     100         0.970123        0.01          0.975008
          6          poly       1         0.966608         0.1          0.979108
          26         poly    1000         0.966608        0.01          0.979304
          32         poly   10000         0.963093        0.01          0.981450
          2        linear     100         0.961336         NaN          0.969342
          3        linear    1000         0.959578         NaN          0.972467
          18         poly     100         0.959578         0.1          0.976176
          33          rbf   10000         0.957821       0.001          0.969929
          14         poly      10         0.956063        0.01          0.964656
          1        linear      10         0.956063         NaN          0.963095
\end{Verbatim}
            
    \hypertarget{third-algorithm-logistic-regression}{%
\subsection{Third Algorithm: Logistic
Regression}\label{third-algorithm-logistic-regression}}

    What is Logistic Regression? 

    For data like this, the linear regression line can't fit: Our hypothesis
function looks like: \includegraphics{attachment:image.png}

    Logistic Regression as the name suggests, seems like a combination of
linear regression and the logistic function. That is in fact true. In
linear regression, we learn theta for our hypothesis function. In
Logistic Regression, we use the sigmoid function to get the
probabilistic value for the predicted label between 0 and 1. If h(x)
\textgreater{}= 0.5, the predicted label is 1 otherwise it is 0.

    We can think of hypothesis, h(x) estimated probability that y=1, on
input thus h(x)=P(y=1\textbar{}x;)

    

    To find , we need to use a cost function and then minimize it. For this
process, we will model the cost function using log loss and then use
gradient descent to minimize it. What all these mean will be clarified
in the document ahead. 

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}134}]:} \PY{n}{clf\PYZus{}logistic} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \hypertarget{hyperparameters}{%
\subsubsection{Hyperparameters:}\label{hyperparameters}}

\begin{itemize}
\tightlist
\item
  C: This regularization parameter is used for generalizing the model,
  and controls the trade off between fitting a small margin hyperplane/
  fitting more points, compared to large margin hyperplane/generalizing
  better.
\item
  penalty (either l1 or l2): This refers to the norm, and how we want to
  apply the regularization
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}135}]:} \PY{n}{clf\PYZus{}logistic}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{clf\PYZus{}logistic}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}135}]:} 0.9090909090909091
\end{Verbatim}
            
    \hypertarget{we-are-going-to-use-gridsearchcv-for-hyperparameter-tuning-and-cross-validation}{%
\subsubsection{We are going to use GridSearchCV for hyperparameter
tuning and cross
validation}\label{we-are-going-to-use-gridsearchcv-for-hyperparameter-tuning-and-cross-validation}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}136}]:} \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{1000}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{penalty}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          \PY{p}{\PYZcb{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}137}]:} \PY{n}{grid} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{clf\PYZus{}logistic}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}jobs} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}138}]:} \PY{n}{grid}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{df}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}138}]:} GridSearchCV(cv=10, error\_score='raise-deprecating',
                 estimator=LogisticRegression(C=1.0, class\_weight=None, dual=False, fit\_intercept=True,
                    intercept\_scaling=1, max\_iter=100, multi\_class='warn',
                    n\_jobs=None, penalty='l2', random\_state=None, solver='warn',
                    tol=0.0001, verbose=0, warm\_start=False),
                 fit\_params=None, iid='warn', n\_jobs=-1,
                 param\_grid=\{'C': [1, 10, 100, 1000], 'penalty': ['l1', 'l2']\},
                 pre\_dispatch='2*n\_jobs', refit=True, return\_train\_score='warn',
                 scoring='accuracy', verbose=0)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}139}]:} \PY{n}{validation\PYZus{}results} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{grid}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{)}
          \PY{n}{validation\PYZus{}results}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}139}]:}    mean\_fit\_time  std\_fit\_time  mean\_score\_time  std\_score\_time param\_C  \textbackslash{}
          0       0.039734      0.008966         0.002240        0.000876       1   
          1       0.015875      0.008287         0.001656        0.000446       1   
          2       0.083949      0.007047         0.001558        0.000477      10   
          3       0.008862      0.002837         0.001266        0.000446      10   
          4       0.261488      0.036284         0.001461        0.000487     100   
          
            param\_penalty                       params  split0\_test\_score  \textbackslash{}
          0            l1    \{'C': 1, 'penalty': 'l1'\}           0.931034   
          1            l2    \{'C': 1, 'penalty': 'l2'\}           0.896552   
          2            l1   \{'C': 10, 'penalty': 'l1'\}           0.982759   
          3            l2   \{'C': 10, 'penalty': 'l2'\}           0.931034   
          4            l1  \{'C': 100, 'penalty': 'l1'\}           0.982759   
          
             split1\_test\_score  split2\_test\_score       {\ldots}         split2\_train\_score  \textbackslash{}
          0           0.879310           0.929825       {\ldots}                   0.945312   
          1           0.896552           0.877193       {\ldots}                   0.925781   
          2           0.931034           0.947368       {\ldots}                   0.970703   
          3           0.896552           0.912281       {\ldots}                   0.951172   
          4           0.931034           0.947368       {\ldots}                   0.972656   
          
             split3\_train\_score  split4\_train\_score  split5\_train\_score  \textbackslash{}
          0            0.945312            0.939453            0.945312   
          1            0.933594            0.916016            0.916016   
          2            0.976562            0.966797            0.966797   
          3            0.951172            0.943359            0.943359   
          4            0.972656            0.972656            0.972656   
          
             split6\_train\_score  split7\_train\_score  split8\_train\_score  \textbackslash{}
          0            0.949219            0.947368            0.941520   
          1            0.925781            0.927875            0.918129   
          2            0.970703            0.962963            0.968811   
          3            0.955078            0.949318            0.939571   
          4            0.978516            0.966862            0.974659   
          
             split9\_train\_score  mean\_train\_score  std\_train\_score  
          0            0.947368          0.946106         0.003374  
          1            0.931774          0.924819         0.006000  
          2            0.970760          0.970126         0.003788  
          3            0.951267          0.948449         0.004533  
          4            0.972710          0.972662         0.003021  
          
          [5 rows x 32 columns]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}140}]:} \PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}score\PYZus{}}\PY{p}{,} \PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{,} \PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}140}]:} (0.9630931458699473,
           \{'C': 100, 'penalty': 'l1'\},
           LogisticRegression(C=100, class\_weight=None, dual=False, fit\_intercept=True,
                     intercept\_scaling=1, max\_iter=100, multi\_class='warn',
                     n\_jobs=None, penalty='l1', random\_state=None, solver='warn',
                     tol=0.0001, verbose=0, warm\_start=False))
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}141}]:} \PY{n}{validation\PYZus{}results}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{param\PYZus{}penalty}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{param\PYZus{}C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}train\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}141}]:}   param\_penalty param\_C  mean\_test\_score  mean\_train\_score
          4            l1     100         0.963093          0.972662
          6            l1    1000         0.961336          0.975591
          7            l2    1000         0.961336          0.971882
          2            l1      10         0.959578          0.970126
          5            l2     100         0.957821          0.966612
          3            l2      10         0.942004          0.948449
          0            l1       1         0.934974          0.946106
          1            l2       1         0.919156          0.924819
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}145}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{confusion\PYZus{}matrix}\PY{p}{,} \PY{n}{recall\PYZus{}score}\PY{p}{,} \PY{n}{precision\PYZus{}score}
\end{Verbatim}


    \hypertarget{model-evaluation}{%
\subsection{Model Evaluation}\label{model-evaluation}}

Now that you have all these nice results, you may also want to look at
how do you wish to evaluate your model? Accuracy is a good metric only
for certain cases, but we can look at the confusion matrix to get a more
holistic idea

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}149}]:} \PY{n}{log\PYZus{}best\PYZus{}clf} \PY{o}{=} \PY{n}{grid}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}
          \PY{n}{y\PYZus{}predicted} \PY{o}{=} \PY{n}{log\PYZus{}best\PYZus{}clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{p}{)}
          \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{log\PYZus{}best\PYZus{}clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}149}]:} array([[85,  2],
                 [ 2, 54]], dtype=int64)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}151}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Recall Score: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{recall\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}predicted}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Recall Score: 0.9642857142857143

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}152}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Precision Score: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{precision\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}predicted}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Precision Score: 0.9642857142857143

    \end{Verbatim}

    \hypertarget{comparison-between-different-models-before-and-after-doing-hyperparameter-tuning}{%
\subsection{Comparison between different models, before and after doing
hyperparameter
tuning}\label{comparison-between-different-models-before-and-after-doing-hyperparameter-tuning}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}82}]:} \PY{n}{improv} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data} \PY{o}{=} \PY{p}{\PYZob{}} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{clf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{naive\PYZus{}bayes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{svm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{logistic\PYZus{}regression}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{before\PYZus{}accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{89.5}\PY{p}{,} \PY{l+m+mf}{93.7}\PY{p}{,} \PY{l+m+mf}{91.6}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{after\PYZus{}accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{90}\PY{p}{,} \PY{l+m+mi}{97}\PY{p}{,} \PY{l+m+mf}{96.3}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{)}
         \PY{n}{improv}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}82}]:}                    clf  before\_accuracy  after\_accuracy
         0          naive\_bayes             89.5            90.0
         1                  svm             93.7            97.0
         2  logistic\_regression             91.6            96.3
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}83}]:} \PY{n}{improv}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{improvement}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{improv}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{after\PYZus{}accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{improv}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{before\PYZus{}accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{improv}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}83}]:}                    clf  before\_accuracy  after\_accuracy  improvement
         0          naive\_bayes             89.5            90.0          0.5
         1                  svm             93.7            97.0          3.3
         2  logistic\_regression             91.6            96.3          4.7
\end{Verbatim}
            
    \hypertarget{how-to-evaluate-a-classifier}{%
\subsection{How to evaluate a
classifier?}\label{how-to-evaluate-a-classifier}}

There's multiple ways in which we can evaluate a classifier. - By
looking at recall score, precision score etc - By looking at the
confusion matrix


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
